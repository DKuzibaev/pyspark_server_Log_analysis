from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
from datetime import datetime
from functools import reduce
from pyspark.sql.functions import avg, col, to_date
from pyspark.sql import functions as F

log_df = spark.read.csv(
    r'/content/web_server_logs.csv',
    header=True,
    inferSchema=True
)

#1
gp_log_df = (
    log_df
    .groupBy("ip")
    .agg(F.count("*").alias('request_count'))
    .orderBy(F.desc('request_count'))
)

print("Top 10 active IP addresses:")
gp_log_df.show()
print("\n")

#2
mthd_log_df = (
    log_df
    .groupBy("method")
    .agg(F.count("*").alias('request_count'))
    .orderBy(F.desc('request_count'))
)

print("Request count by HTTP method:")
mthd_log_df.show()
print("\n")

#3
method_404_count = log_df.filter(log_df["response_code"] == 404).count()
print(f"Number of 404 response codes: {method_404_count}\n")
print("\n")

#4
size_by_date_df = (
    log_df
    .withColumn("date", to_date("timestamp"))
    .groupBy("date")
    .agg(F.sum("response_size").alias("total_response_size"))
    .orderBy("date")
)

print("Total response size by day:")
size_by_date_df.show()
print("\n")
